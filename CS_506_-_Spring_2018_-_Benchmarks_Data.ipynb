{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates the URLs for ma-appellatecourts.org which we will want to download. There is probably no need to run this again unless we need to capture more current cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_url(base, case_type, year, number):\n",
    "    \"\"\"\n",
    "    Given a case type, year, and number, generate the URL for it on the MA Appellate Court website\n",
    "    \n",
    "    Input:\n",
    "        base: base of URL\n",
    "        case type: Type of case in J, P, SJ, and SJC\n",
    "        year: Year of case\n",
    "        number: Case number\n",
    "    Output:\n",
    "        URL to case\n",
    "    \"\"\"\n",
    "    \n",
    "    if case_type in [\"J\", \"P\"]: # format: http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=1999-P-1\n",
    "        return base + str(year) + \"-\" + case_type + \"-\" + str(number)\n",
    "    elif case_type in [\"SJ\"]: # format: http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=SJ-2011-0500\n",
    "        return base + case_type + \"-\" + str(year) + \"-\" + str(number)\n",
    "    else: #http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=SJC-10108\n",
    "        return base + case_type + \"-\" + str(number)\n",
    "\n",
    "base = \"http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=\"\n",
    "\n",
    "# Number of J cases by year\n",
    "j_limits = {}\n",
    "#j_limits[2008] = 547\n",
    "#j_limits[2009] = 565\n",
    "#j_limits[2010] = 589\n",
    "#j_limits[2011] = 550\n",
    "#j_limits[2012] = 482\n",
    "#j_limits[2013] = 568\n",
    "#j_limits[2014] = 514\n",
    "#j_limits[2015] = 527\n",
    "#j_limits[2016] = 539\n",
    "j_limits[2017] = 581\n",
    "j_limits[2018] = 107\n",
    "\n",
    "# Number of P cases by year\n",
    "p_limits = {}\n",
    "#p_limits[2008] = 2156\n",
    "#p_limits[2009] = 2354\n",
    "#p_limits[2010] = 2281\n",
    "#p_limits[2011] = 2182\n",
    "#p_limits[2012] = 2023\n",
    "#p_limits[2013] = 2031\n",
    "#p_limits[2014] = 1995\n",
    "#p_limits[2015] = 1755\n",
    "#p_limits[2016] = 1758\n",
    "p_limits[2017] = 1634\n",
    "p_limits[2018] = 343\n",
    "\n",
    "# Number of SJ cases by year\n",
    "sj_limits = {}\n",
    "#sj_limits[2008] = 575\n",
    "#sj_limits[2009] = 668\n",
    "#sj_limits[2010] = 586\n",
    "#sj_limits[2011] = 555\n",
    "#sj_limits[2012] = 521\n",
    "#sj_limits[2013] = 503\n",
    "#sj_limits[2014] = 529\n",
    "#sj_limits[2015] = 561\n",
    "#sj_limits[2016] = 536\n",
    "sj_limits[2017] = 519\n",
    "sj_limits[2018] = 105\n",
    "\n",
    "# Lower and upper limit of SJC case numbers within current window (2008-2018)\n",
    "sjc_lower = 10108\n",
    "sjc_upper = 12497\n",
    "\n",
    "# Links to all cases\n",
    "links = []\n",
    "\n",
    "# Generate all the links based on the above controls\n",
    "for year, n in j_limits.items():\n",
    "    for i in range(n):\n",
    "        links.append(generate_url(base, \"J\", year, i + 1))\n",
    "for year, n in p_limits.items():\n",
    "    for i in range(n):\n",
    "        links.append(generate_url(base, \"P\", year, i + 1))\n",
    "for year, n in sj_limits.items():\n",
    "    for i in range(n):\n",
    "        links.append(generate_url(base, \"SJ\", year, i + 1))\n",
    "for i in range(sjc_upper - sjc_lower):\n",
    "    links.append(generate_url(base, \"SJC\", 0, i + sjc_lower + 1))\n",
    "\n",
    "with open(\"urls_total.txt\", \"w\") as text_file:\n",
    "    for link in links:\n",
    "        print(link, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes the URLs (from a different file than the one written to above- this way, we can limit the scope if we so desire) and pulls the text down for us to keep. Be advised that the operation succeeds even if the page we pull down is \"hey you've been blocked so GTFO\", so be sure to remove any files that are downloaded and are too small to be court cases (in my case, the minimum size is 7 KB, which is a \"this number wasn't found\"; most cases are much larger. However, the 'blocked' responses are 3 KB, but your ISP may vary. Still these are probably always smaller than actual court cases). The main loop which controls the page reads also checks if we have the case before we pull it, so there's no need to worry about pulling duplicates (but if we pull a 'blocked' response, we do need to add it back)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "def get_page_text(url):\n",
    "    \"\"\"\n",
    "    Given a URL, return the text content\n",
    "    \n",
    "    Input:\n",
    "        url: a string representing a URL\n",
    "    Output:\n",
    "        the content of said URL\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\"Connection\" : \"close\"}\n",
    "    page = \"\"\n",
    "    while (page == \"\"):\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            page = requests.get(url)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "    return page.text\n",
    "\n",
    "def write_page_text(url, text):\n",
    "    \"\"\"\n",
    "    Write a page's text content to a file\n",
    "    \n",
    "    Input:\n",
    "        url: a string representing the source of the text\n",
    "        text: the text content\n",
    "    Output:\n",
    "        the filename under which the content was written\n",
    "    \"\"\"\n",
    "    filename = r'C:\\Users\\jcraver\\Desktop\\BENCHMARKS\\%s.html' % url.split('dno=')[-1]\n",
    "    with open(filename, \"w\") as text_file:\n",
    "        print(text, file=text_file)\n",
    "    return filename\n",
    "\n",
    "links = set([])\n",
    "folder = r'C:\\Users\\jcraver\\Desktop\\BENCHMARKS'\n",
    "\n",
    "with open(\"urls_todo.txt\", \"r\") as text_file:\n",
    "    for line in text_file:\n",
    "        links.add(line.strip())\n",
    "\n",
    "# Get files that have already been done\n",
    "done = set([])\n",
    "for file in os.listdir(folder):\n",
    "    done.add(file)\n",
    "\n",
    "# Starting from where we left off, pull down pages and write them\n",
    "# This is to limit what we do at once (if desired)\n",
    "countdown = 600\n",
    "processed = []\n",
    "for link in links:\n",
    "    processed.append(link)\n",
    "    # Don't download a file we already have\n",
    "    if (link.split('dno=')[-1] + '.html') in done:\n",
    "        continue\n",
    "    write_page_text(link, get_page_text(link))\n",
    "    countdown -= 1\n",
    "    #print(link)\n",
    "    if countdown <= 0:\n",
    "        break\n",
    "\n",
    "# Write down what we've done\n",
    "for link in processed:\n",
    "    links.remove(link)\n",
    "with open(\"urls_todo.txt\", \"w\") as text_file:\n",
    "    for link in links:\n",
    "        print(link, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads in the HTML files from the hard drive (not the web). This will need some work to read the docket entries, but that work can all be done within the scrape_page method; further web queries are probably unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_page(filename):\n",
    "    \"\"\"\n",
    "    Open the file and Soup it as beautifully as possible\n",
    "    \n",
    "    Input:\n",
    "        filename: The filename to parse\n",
    "    Output:\n",
    "        A dictionary of the items found in the case page\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(filename), 'html.parser')\n",
    "    info = {}\n",
    "    \n",
    "    # Get case tags\n",
    "    header = soup.find('td', class_=\"largefont\")\n",
    "    if len(list(soup.find_all(\"td\", align=\"center\"))) < 2:\n",
    "        return {}\n",
    "    center_cells = list(soup.find_all(\"td\", align=\"center\")[1].stripped_strings)\n",
    "    info[\"Court Type\"] = header.b.contents[0]\n",
    "    info[\"Panel\"] = header.b.contents[1].text\n",
    "    info[\"Case Name\"] = center_cells[0]\n",
    "    info[\"Case Id\"] = center_cells[-1]\n",
    "    \n",
    "    # Get court tags\n",
    "    tables = soup.find_all(\"table\", class_=\"lightborder\")\n",
    "    attr_table = tables[0]\n",
    "    for row in attr_table.find_all(\"tr\", valign=\"top\"): \n",
    "        items = row.find_all(\"b\")\n",
    "        for item in items:\n",
    "            k = item.text\n",
    "            v = item.next.next.text.strip()\n",
    "            info[k] = v\n",
    "    \n",
    "    # Get parties\n",
    "    parties_table = soup.find(\"table\", class_=\"lightborder\", cellpadding=\"5\")\n",
    "    if parties_table is None:\n",
    "        return {}\n",
    "    p_k = set([])\n",
    "    for row in parties_table.find_all(\"tr\")[1:]:\n",
    "        k = row.b.nextSibling.next.strip().split('/')[0]\n",
    "        v = row.b.text.strip()\n",
    "\n",
    "        if k in info:\n",
    "            info[k].append(v)\n",
    "        else: \n",
    "            info[k] = [v]\n",
    "            p_k.add(k)\n",
    "    for k in p_k:\n",
    "        info[k] = \", \".join(info[k])\n",
    "    \n",
    "    # Get docket entries\n",
    "    if len(tables) >= 3:\n",
    "        docket = []\n",
    "        docket_table = tables[2]\n",
    "        for row in docket_table.find_all(\"tr\")[1:]:\n",
    "            items = row.find_all(\"td\")\n",
    "            date = items[0].text.strip()\n",
    "            entry = \"\"\n",
    "            if len(items) >= 3:\n",
    "                entry = str(items[2].text.strip())\n",
    "                entry = re.sub(r\"\\s+\", \" \", entry, flags=re.UNICODE)\n",
    "            docket.append(\"%s ::: %s\" %(date, entry))\n",
    "        info['Docket Entries'] = \" %%% \".join(docket)\n",
    "    \n",
    "    return info\n",
    "\n",
    "base = \"http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=\"\n",
    "folder = r'C:\\Users\\jcraver\\Desktop\\BENCHMARKS'\n",
    "cases = []\n",
    "keys = set([])\n",
    "\n",
    "# Read in all the downloaded pages and print / process them\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".html\"):\n",
    "        fullname = os.path.join(folder, file)\n",
    "        case = scrape_page(fullname)\n",
    "        case['URL'] = base + file\n",
    "        keys.update(case.keys())\n",
    "        cases.append(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "\n",
    "# Write out the csv\n",
    "with open('cases.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code does not really work. It was intended to download the Lexis cases, but that has not proved fruitful. Left for posterity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_page_text_with_session(url):\n",
    "    \"\"\"\n",
    "    Given a URL, return the text content\n",
    "    \n",
    "    Input:\n",
    "        url: a string representing a URL\n",
    "    Output:\n",
    "        the content of said URL\n",
    "    \"\"\"\n",
    "    \n",
    "    s = requests.Session()\n",
    "    \n",
    "    headers = {\"Connection\" : \"close\"}\n",
    "    page = \"\"\n",
    "    cookies = {}\n",
    "    while (page == \"\"):\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            page = s.get(url)\n",
    "            cookies = dict(page.cookies)\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "    print(page.text)\n",
    "    print(\"roflcopter\")\n",
    "    page = \"\"\n",
    "    while (page == \"\"):\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            page = s.post(url, verify=False, cookies=cookies)\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "    print(page.text)\n",
    "\n",
    "url = 'https://advance.lexis.com/container/?pdmfid=1000516&crid=3df8cfbc-a7e7-4a44-a279-1c90333eda06&pdsearchterms=SJC-11926&pdstartin=hlct%3A1%3A1&pdtypeofsearch=searchboxclick&pdsearchtype=SearchBox&pdqttype=and&pdsf=&pdquerytemplateid=urn%3Aquerytemplate%3A3d60bd7967c1b2f691b2d991f141df6a~%5ESources&pdsourcetype=all&pdparentqt=urn%3Aquerytemplate%3A3d60bd7967c1b2f691b2d991f141df6a~%5ESources&config=00JAA3MTBiMzg0Yi1iY2IxLTQ0ODktYWNlMi02MWEzYjExMThhY2UKAFBvZENhdGFsb2edTHaWI0HCIdzKAqqpyUcJ&ecomp=kg2_9kk&earg=pdsf&prid=a21a0be5-aa4f-4d73-af33-a73415932187'\n",
    "\n",
    "#print(get_page_text_with_session(url))\n",
    "\n",
    "def get_lexis_session(url):\n",
    "    s = requests.session()\n",
    "    \n",
    "    return s.get(url)\n",
    "\n",
    "homeurl = 'https://advance.lexis.com/container?config=0152JAAzMjQ0NTBmOS05ZmRhLTQ4NmQtOTk0NC1lNmQ1MzdmYmZlN2UKAFBvZENhdGFsb2dFqubrO2osfYY9MSag4F39&crid=bc684bfd-28bb-40c7-8178-f00690c843ed&prid=a21a0be5-aa4f-4d73-af33-a73415932187'\n",
    "r = get_lexis_session(homeurl)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This would be where we put code to analyze/cluster our ma-appellatecourts.org data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement\n",
    "\n",
    "# Run LSA on the docket entries (I estimate 2 components, but I haven't checked yet)\n",
    "# Cluster (k-means?) on tfidf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be where we put code to analyze/cluster our Lexis opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: implement\n",
    "\n",
    "# Run LSA on the opinions (I estimate 2 components, but I haven't checked yet)\n",
    "# Cluster (k-means?) on tfidf scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
