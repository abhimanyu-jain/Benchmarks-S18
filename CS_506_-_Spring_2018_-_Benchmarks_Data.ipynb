{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates the URLs for ma-appellatecourts.org which we will want to download. There is probably no need to run this again unless we need to capture more current cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_url(base, case_type, year, number):\n",
    "    \"\"\"\n",
    "    Given a case type, year, and number, generate the URL for it on the MA Appellate Court website\n",
    "    \n",
    "    Input:\n",
    "        base: base of URL\n",
    "        case type: Type of case in J, P, SJ, and SJC\n",
    "        year: Year of case\n",
    "        number: Case number\n",
    "    Output:\n",
    "        URL to case\n",
    "    \"\"\"\n",
    "    \n",
    "    if case_type in [\"J\", \"P\"]: # format: http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=1999-P-1\n",
    "        return base + str(year) + \"-\" + case_type + \"-\" + str(number)\n",
    "    elif case_type in [\"SJ\"]: # format: http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=SJ-2011-0500\n",
    "        return base + case_type + \"-\" + str(year) + \"-\" + str(number)\n",
    "    else: #http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=SJC-10108\n",
    "        return base + case_type + \"-\" + str(number)\n",
    "\n",
    "base = \"http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=\"\n",
    "\n",
    "# Number of J cases by year\n",
    "j_limits = {}\n",
    "j_limits[2008] = 547\n",
    "j_limits[2009] = 565\n",
    "j_limits[2010] = 589\n",
    "j_limits[2011] = 550\n",
    "j_limits[2012] = 482\n",
    "j_limits[2013] = 568\n",
    "j_limits[2014] = 514\n",
    "j_limits[2015] = 527\n",
    "j_limits[2016] = 539\n",
    "j_limits[2017] = 581\n",
    "j_limits[2018] = 130\n",
    "\n",
    "# Number of P cases by year\n",
    "p_limits = {}\n",
    "p_limits[2008] = 2156\n",
    "p_limits[2009] = 2354\n",
    "p_limits[2010] = 2281\n",
    "p_limits[2011] = 2182\n",
    "p_limits[2012] = 2023\n",
    "p_limits[2013] = 2031\n",
    "p_limits[2014] = 1995\n",
    "p_limits[2015] = 1755\n",
    "p_limits[2016] = 1758\n",
    "p_limits[2017] = 1634\n",
    "p_limits[2018] = 365\n",
    "\n",
    "# Number of SJ cases by year\n",
    "sj_limits = {}\n",
    "sj_limits[2008] = 575\n",
    "sj_limits[2009] = 668\n",
    "sj_limits[2010] = 586\n",
    "sj_limits[2011] = 555\n",
    "sj_limits[2012] = 521\n",
    "sj_limits[2013] = 503\n",
    "sj_limits[2014] = 529\n",
    "sj_limits[2015] = 561\n",
    "sj_limits[2016] = 536\n",
    "sj_limits[2017] = 511\n",
    "sj_limits[2018] = 125\n",
    "\n",
    "# Lower and upper limit of SJC case numbers within current window (2008-2018)\n",
    "sjc_lower = 10108\n",
    "sjc_upper = 12510\n",
    "\n",
    "# Links to all cases\n",
    "links = []\n",
    "\n",
    "# Generate all the links based on the above controls\n",
    "for year, n in j_limits.items():\n",
    "    for i in range(n):\n",
    "        links.append(generate_url(base, \"J\", year, i + 1))\n",
    "for year, n in p_limits.items():\n",
    "    for i in range(n):\n",
    "        links.append(generate_url(base, \"P\", year, i + 1))\n",
    "for year, n in sj_limits.items():\n",
    "    for i in range(n):\n",
    "        links.append(generate_url(base, \"SJ\", year, i + 1))\n",
    "for i in range(sjc_upper - sjc_lower):\n",
    "    links.append(generate_url(base, \"SJC\", 0, i + sjc_lower + 1))\n",
    "\n",
    "with open(\"urls_todo.txt\", \"w\") as text_file:\n",
    "    for link in links:\n",
    "        print(link, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes the URLs (from a different file than the one written to above- this way, we can limit the scope if we so desire) and pulls the text down for us to keep. Be advised that the operation succeeds even if the page we pull down is \"you've been blocked\", so be sure to remove any files that are downloaded and are too small to be court cases (in my case, the minimum size is 7 KB, which is a \"this number wasn't found\"; most cases are much larger. However, the 'blocked' responses are 3 KB, but your ISP may vary. Still these are probably always smaller than actual court cases). The main loop which controls the page reads also checks if we have the case before we pull it, so there's no need to worry about pulling duplicates (but if we pull a 'blocked' response, we do need to add it back)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "\n",
    "# List of user agents to choose from\n",
    "useragents = '''Mozilla/5.0 (Windows; U; ; en-NZ) AppleWebKit/527  (KHTML, like Gecko, Safari/419.3) Arora/0.8.0\n",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Avant Browser; Avant Browser; .NET CLR 1.0.3705; .NET CLR 1.1.4322; Media Center PC 4.0; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30)\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.8 (KHTML, like Gecko) Beamrise/17.2.0.9 Chrome/17.0.939.0 Safari/535.8\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/28.0.1469.0 Safari/537.36\n",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/28.0.1469.0 Safari/537.36\n",
    "Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36\n",
    "Mozilla/5.0 (Windows NT 6.0; rv:14.0) Gecko/20100101 Firefox/14.0.1\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:15.0) Gecko/20120427 Firefox/15.0a1\n",
    "Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0) Gecko/16.0 Firefox/16.0\n",
    "Mozilla/5.0 (Windows NT 6.2; rv:19.0) Gecko/20121129 Firefox/19.0\n",
    "Mozilla/5.0 (Windows NT 6.1; rv:21.0) Gecko/20130401 Firefox/21.0\n",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/25.0\n",
    "iTunes/9.0.2 (Windows; N)\n",
    "Mozilla/5.0 (compatible; Konqueror/4.5; Windows) KHTML/4.5.4 (like Gecko)\n",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; Maxthon 2.0)\n",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/533.1 (KHTML, like Gecko) Maxthon/3.0.8.2 Safari/533.1\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML like Gecko) Maxthon/4.0.0.2000 Chrome/22.0.1229.79 Safari/537.1\n",
    "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\n",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\n",
    "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.04506.648; .NET CLR 3.5.21022; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)\n",
    "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0)\n",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Trident/4.0)\n",
    "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)\n",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Trident/5.0)\n",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\n",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.2; Trident/5.0)\n",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.2; WOW64; Trident/5.0)\n",
    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; Media Center PC 6.0; InfoPath.3; MS-RTC LM 8; Zune 4.7)\n",
    "Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)\n",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; Trident/6.0)\n",
    "Mozilla/5.0 (compatible; MSIE 10.6; Windows NT 6.1; Trident/5.0; InfoPath.2; SLCC1; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; .NET CLR 2.0.50727) 3gpp-gba UNTRUSTED/1.0\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\n",
    "Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko\n",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.3; Trident/7.0; .NET4.0E; .NET4.0C)\n",
    "Opera/9.80 (Windows NT 6.1; U; en) Presto/2.7.62 Version/11.01\n",
    "Opera/9.80 (Windows NT 6.0) Presto/2.12.388 Version/12.14\n",
    "Opera/9.80 (Windows NT 6.1; WOW64) Presto/2.12.388 Version/12.16\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.12 Safari/537.36 OPR/14.0.1116.4\n",
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.29 Safari/537.36 OPR/15.0.1147.24 (Edition Next)\n",
    "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.57 Safari/537.36 OPR/18.0.1284.49\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.76 Safari/537.36 OPR/19.0.1326.56\n",
    "Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/533.17.8 (KHTML, like Gecko) Version/5.0.1 Safari/533.17.8\n",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/533.19.4 (KHTML, like Gecko) Version/5.0.2 Safari/533.18.5\n",
    "Mozilla/5.0 (Windows; U; Windows NT 6.2; es-US ) AppleWebKit/540.0 (KHTML like Gecko) Version/6.0 Safari/8900.00\n",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-GB; rv:1.9.1.17) Gecko/20110123 (like Firefox/3.x) SeaMonkey/2.0.12\t\t\n",
    "Mozilla/5.0 (Windows NT 5.2; rv:10.0.1) Gecko/20100101 Firefox/10.0.1 SeaMonkey/2.7.1\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:12.0) Gecko/20120422 Firefox/12.0 SeaMonkey/2.9'''.split('\\n')\n",
    "\n",
    "def get_page_text(url, useragent, sleep_timings = [2, 3, 5, 8], exception_timings = [5, 10, 15]):\n",
    "    \"\"\"\n",
    "    Given a URL, return the text content\n",
    "    \n",
    "    Input:\n",
    "        url: a string representing a URL\n",
    "        useragent: our totally not fake id, officer\n",
    "        sleep_timings: list of possible numbers of seconds to wait between requests\n",
    "        exception_timings: list of possible numbers of seconds to wait between exceptions before retrying\n",
    "    Output:\n",
    "        the content of said URL\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the header\n",
    "    headers = {\"Connection\": \"close\", \"user-agent\": useragent}\n",
    "    \n",
    "    # Request until we have a result\n",
    "    page = \"\"\n",
    "    while (page == \"\"):\n",
    "        try:\n",
    "            time.sleep(random.choice(sleep_timings))\n",
    "            page = requests.get(url)\n",
    "        except:\n",
    "            print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "            time.sleep(random.choice(exception_timings))\n",
    "            continue\n",
    "    \n",
    "    # Keep the page text\n",
    "    return page.text\n",
    "\n",
    "def write_page_text(url, text):\n",
    "    \"\"\"\n",
    "    Write a page's text content to a file\n",
    "    \n",
    "    Input:\n",
    "        url: a string representing the source of the text\n",
    "        text: the text content\n",
    "    Output:\n",
    "        the filename under which the content was written\n",
    "    \"\"\"\n",
    "    filename = r'C:\\Users\\jcraver\\Desktop\\BENCHMARKS\\%s.html' % url.split('dno=')[-1]\n",
    "    with open(filename, \"w\") as text_file:\n",
    "        print(text, file=text_file)\n",
    "    return filename\n",
    "\n",
    "links = set([])\n",
    "folder = r'C:\\Users\\jcraver\\Desktop\\BENCHMARKS'\n",
    "\n",
    "with open(\"urls_todo.txt\", \"r\") as text_file:\n",
    "    for line in text_file:\n",
    "        links.add(line.strip())\n",
    "\n",
    "# Get files that have already been done\n",
    "done = set([])\n",
    "for file in os.listdir(folder):\n",
    "    done.add(file)\n",
    "\n",
    "# Starting from where we left off, pull down pages and write them\n",
    "# This is to limit what we do at once (if desired)\n",
    "countdown = 1000\n",
    "processed = []\n",
    "for link in links:\n",
    "    processed.append(link)\n",
    "    # Don't download a file we already have\n",
    "    if (link.split('dno=')[-1] + '.html') in done:\n",
    "        continue\n",
    "    write_page_text(link, get_page_text(link, random.choice(useragents), [3, 5, 8, 13], [10, 20, 30]))\n",
    "    countdown -= 1\n",
    "    #print(link)\n",
    "    if countdown <= 0:\n",
    "        break\n",
    "\n",
    "# Write down what we've done\n",
    "for link in processed:\n",
    "    links.remove(link)\n",
    "with open(\"urls_todo.txt\", \"w\") as text_file:\n",
    "    for link in links:\n",
    "        print(link, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads in the HTML files from the hard drive (not the web). This will need some work to read the docket entries, but that work can all be done within the scrape_page method; further web queries are probably unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_mac_page(filename):\n",
    "    \"\"\"\n",
    "    Open the MA Appellate Court html file and Soup it as beautifully as possible\n",
    "    \n",
    "    Input:\n",
    "        filename: The filename to parse\n",
    "    Output:\n",
    "        A dictionary of the items found in the case page\n",
    "        A list of dated docket entries\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(filename), 'html.parser')\n",
    "    info = {}\n",
    "    \n",
    "    # Get case tags\n",
    "    header = soup.find('td', class_=\"largefont\")\n",
    "    if len(list(soup.find_all(\"td\", align=\"center\"))) < 2:\n",
    "        return {}, []\n",
    "    center_cells = list(soup.find_all(\"td\", align=\"center\")[1].stripped_strings)\n",
    "    info[\"Court Type\"] = header.b.contents[0]\n",
    "    info[\"Panel\"] = header.b.contents[1].text\n",
    "    info[\"Case Name\"] = center_cells[0]\n",
    "    info[\"Case Id\"] = center_cells[-1]\n",
    "    \n",
    "    # Get court tags\n",
    "    tables = soup.find_all(\"table\", class_=\"lightborder\")\n",
    "    attr_table = tables[0]\n",
    "    for row in attr_table.find_all(\"tr\", valign=\"top\"): \n",
    "        items = row.find_all(\"b\")\n",
    "        for item in items:\n",
    "            k = item.text\n",
    "            v = item.next.next.text.strip()\n",
    "            info[k] = v\n",
    "    \n",
    "    # Get parties\n",
    "    parties_table = soup.find(\"table\", class_=\"lightborder\", cellpadding=\"5\")\n",
    "    if parties_table is None:\n",
    "        return {}, []\n",
    "    p_k = set([])\n",
    "    for row in parties_table.find_all(\"tr\")[1:]:\n",
    "        k = row.b.nextSibling.next.strip().split('/')[0]\n",
    "        v = row.b.text.strip()\n",
    "\n",
    "        if k in info:\n",
    "            info[k].append(v)\n",
    "        else: \n",
    "            info[k] = [v]\n",
    "            p_k.add(k)\n",
    "    for k in p_k:\n",
    "        info[k] = \", \".join(info[k])\n",
    "    \n",
    "    # Get docket entries\n",
    "    docket = []\n",
    "    if len(tables) >= 3:\n",
    "        docket_table = tables[2]\n",
    "        for row in docket_table.find_all(\"tr\")[1:]:\n",
    "            items = row.find_all(\"td\")\n",
    "            date = items[0].text.strip()\n",
    "            entry = \"\"\n",
    "            if len(items) >= 3:\n",
    "                entry = str(items[2].text.strip())\n",
    "                entry = re.sub(r\"\\s+\", \" \", entry, flags=re.UNICODE)\n",
    "            docket.append([info['Case Id'], date, entry])\n",
    "    \n",
    "    return info, docket\n",
    "\n",
    "base = \"http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=\"\n",
    "folder = r'C:\\Users\\jcraver\\Desktop\\BENCHMARKS'\n",
    "cases = []\n",
    "dockets = []\n",
    "keys = set([])\n",
    "\n",
    "# Read in all the downloaded pages and print / process them\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".html\"):\n",
    "        fullname = os.path.join(folder, file)\n",
    "        case, docket = scrape_mac_page(fullname)\n",
    "        case['URL'] = base + file\n",
    "        keys.update(case.keys())\n",
    "        cases.append(case)\n",
    "        dockets += docket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pro Se Third-party defendant', 'Route to SJC', 'Court Type', 'Clerk for Commonwealth', 'Respondent', 'Pro Se Guardian ad litem', 'Out-of-state counsel', 'Pro Se Other', 'CPCS administrator', 'TC Number', 'cmt', 'Out of state counsel', 'Minor', 'Amicus (plaintiff)', 'Student appearing under SJC Rule 3:03', 'amid', 'Nature', 'Other Appellee', 'Out-of-state attorney', 'e3pp', 'Pro Se Respondent', 'Amicus', 'Third-party Defendant', 'Pro Se Amicus (plaintiff)', 'Massacusetts Correctional Institution', 'Nominal Party', 'Out-of-state counsel for defendant', 'Invited to file amicus brief', 'Pro Se Third-party plaintiff', 'Single Justice', 'Pro Se poth', 'Massachusetts attorney pending BBO number', 'Pro Se pet', 'clk', 'Argued/Submitted', 'Student Attorney purs to SJC 3:03', 'Receiver', 'DAR/FAR Number', 'Pro Se Amicus', '(Lower Court: jury)', 'Plaintiff', 'Third-party defendant', 'Panel', '3pp', 'rplf', 'TC Entry Date', '(Lower Court: Jury)', 'scom', 'Pro Se Defendant', 'amip', 'Out-of-state counsel for plaintiff', 'Out of state counsel (plaintiff)', 'Brief Due', 'Hearing Officer', 'Out-of-state counsel for amicus', 'pet', 'Quorum', 'Argued Date', 'Petitioner', 'Lower Ct Number', 'Out of state counsel (defendant)', 'Massachusetts Correctional Institution', 'Decision Date', 'Pro Se Student Atty for Commonwealth', 'Guardian ad litem', 'Nominal Defendant', 'Sub-Nature', 'Voluntary Dismissal as Party', 'Other interested party', 'comp', 'Intervener', 'Pro Se Other Appellant', 'Citation', 'SJ Number', 'Third-party plaintiff', 'Case Status', 'Lower Ct Judge', 'Defendant', '3pd', 'Lower Court', 'Pro Se Other interested party', 'Pro Se Intervener', 'Other', 'Other Appellant', 'Case Name', 'Party Name Impounded', 'AC/SJ Number', 'FAR Number', 'Pro Se Plaintiff', 'URL', 'res', 'Pro Se Petitioner', 'Status Date', 'Intervener-Petitioner', 'Pet Role Below', 'Student Atty for Commonwealth', 'Pro Se Other Appellee', 'Out-of-state counsel for appellant', 'Amicus (defendant)', 'Brief Status', 'SJC Number', 'resp', 'Transferred to Court', 'Out-of-state counsel for appellee', 'Student attorney purs to SJC 3:03', 'Pro Se Third-party Defendant', 'Witness', 'Case Id', 'Third party plaintiff', 'Case Type', 'Student appearing under Rule 3:03', 'Entry Date', 'Appellant'}\n"
     ]
    }
   ],
   "source": [
    "import unicodecsv as csv\n",
    "\n",
    "print(keys)\n",
    "\n",
    "# Write out the case csv\n",
    "with open('cases.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(cases)\n",
    "\n",
    "# Write out the case csv\n",
    "with open('dockets.csv', 'wb') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Case ID', 'Date', 'Entry'])\n",
    "    for entry in dockets:\n",
    "        writer.writerow(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some labeling and graphing utilities I developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ENGLISH_STOP_WORDS = frozenset([\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"])\n",
    "\n",
    "def extract_groupings(X, corpus, k, labels):\n",
    "    \"\"\"\n",
    "    Return data points and metadata grouped by cluster\n",
    "    \n",
    "    Input:\n",
    "        X = data points\n",
    "        corpus = the reviews for each restaurant\n",
    "        k = number of clusters\n",
    "        labels = cluster number of each point in X\n",
    "    Output:\n",
    "        Two list of lists, where the ith sublist is all data points (or title) in the ith cluster\n",
    "    \"\"\"\n",
    "    # Grouping of points by cluster and metadata by cluster\n",
    "    groupings = [[] for i in range(k)]\n",
    "    corpora = [[] for i in range(k)]\n",
    "    \n",
    "    # Sort each point (and associated metadata) into bins for each cluster label\n",
    "    for i in range(len(X)):\n",
    "        label = labels[i]\n",
    "        \n",
    "        # Add this point to the cluster based on its label\n",
    "        groupings[label].append(X[i])\n",
    "        corpora[label].append(corpus[i])\n",
    "    \n",
    "    # Score the value of each term within the groupings and get the most meaningful terms for each cluster\n",
    "    titles = ['' for i in range(k)]\n",
    "    for i in range(k):\n",
    "        tiv = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, min_df=(0.01 * k), ngram_range=(1, 3))\n",
    "        tiv.fit(corpora[i])\n",
    "        indices = np.argsort(tiv.idf_)[::-1]\n",
    "        titles[i] = [tiv.get_feature_names()[i] for i in indices[:10]]\n",
    "    \n",
    "    return groupings, titles\n",
    "\n",
    "def plot_clustering(k, groupings, labels, title):\n",
    "    \"\"\"\n",
    "    Plot the given k clusters on a 16x16 plot.\n",
    "    \n",
    "    Input:\n",
    "        k = the number of clusters\n",
    "        groupings = list of lists corresponding to the points in each cluster\n",
    "        labels = the title of each cluster\n",
    "        title = the title of the plot\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # This size seems quite reasonable/readable\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.axes().set_aspect('equal')\n",
    "    \n",
    "    # Store the plot results so we can label them later\n",
    "    legend = []\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i in range(k):\n",
    "        plot = plt.scatter([entry[1] for entry in groupings[i]], [entry[0] for entry in groupings[i]], alpha=0.5)#, color=colors[i], marker=markers[i], s=8)\n",
    "        legend.append(plot)\n",
    "    \n",
    "    # Label each cluster\n",
    "    plt.legend(legend, labels, fancybox=True, framealpha=0.5)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This would be where we put code to analyze/cluster our ma-appellatecourts.org data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import mixture\n",
    "from sklearn import metrics\n",
    "import scipy.cluster.hierarchy as hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "corpus_dockets = [re.sub(r'\\d', '', case['Docket Entries']) for case in cases if 'Docket Entries' in case and len(case['Docket Entries']) > 0 and 'Case Type' in case and case['Case Type'] == 'Criminal' and (case['Case Status'].find('Rescript') >= 0) and case['Docket Entries'].lower().find('reversed') >= 0]   \n",
    "\n",
    "# Plot histogram of lower court judges\n",
    "#judges = []\n",
    "#for case in cases:\n",
    "#    if 'Docket Entries' in case and len(case['Docket Entries']) > 0 and 'Case Type' in case and case['Case Type'] == 'Criminal' and (case['Case Status'].find('Rescript') >= 0):\n",
    "#        if 'Lower Ct Judge' in case and len(case['Lower Ct Judge']) > 0:\n",
    "#            judges.append(case['Lower Ct Judge'])\n",
    "#judge_counts = Counter(judges)\n",
    "#common_judges = [[j[0]] * j[1] for j in judge_counts.most_common(20)]\n",
    "#common_judges = [x for y in common_judges for x in y]\n",
    "#common_judge_counts = Counter(common_judges)\n",
    "#df = pandas.DataFrame.from_dict(common_judge_counts, orient='index')\n",
    "#df.plot(kind='bar')\n",
    "\n",
    "# Compute the tf-idf scores of the opinions\n",
    "# Using up to trigrams to account for adverbs and for legal terms\n",
    "fe_tfv = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, min_df=0.01, max_df = 0.5, ngram_range=(1, 3))\n",
    "tfidf_dockets = fe_tfv.fit_transform(corpus_dockets)\n",
    "\n",
    "# Compute the LSA of the scored opinions\n",
    "# After 4 components, we don't get much more ROI (plot leading to this conclusion is commented out below)\n",
    "# min/max document frequency play a huge role here\n",
    "n_c = 6\n",
    "dc_tsvd = TruncatedSVD(n_components=n_c)\n",
    "lsa_dockets = dc_tsvd.fit_transform(tfidf_dockets)\n",
    "\n",
    "# This is the code we would use to graph the singular values\n",
    "#fe_tfv = TfidfVectorizer(stop_words='english', min_df = 0.01, max_df = 0.5)\n",
    "#tfidf_dockets = fe_tfv.fit_transform(corpus_dockets)\n",
    "#dc_tsvd = TruncatedSVD(n_components=50)\n",
    "#lsa_dockets = dc_tsvd.fit_transform(tfidf_dockets)\n",
    "#plt.plot(range(1,51), dc_tsvd.singular_values_)\n",
    "#print(dc_tsvd.singular_values_)\n",
    "\n",
    "# Defining terms for each component\n",
    "#terms = fe_tfv.get_feature_names()\n",
    "#for i in range(n_c):\n",
    "#    top = np.argsort(dc_tsvd.components_[i])\n",
    "#    topterms = [terms[top[f]] for f in range(25)]\n",
    "#    print (i, topterms)\n",
    "\n",
    "# Cluster via GMM\n",
    "# Best silhouette score is found with k=3, with 5 also being a local maximum\n",
    "# Plot leading to this conclusion commented out below\n",
    "#k = 5\n",
    "#gmm_dockets = mixture.GaussianMixture(n_components=k).fit(lsa_dockets)\n",
    "#groupings_dockets, titles_dockets = extract_groupings(lsa_dockets, corpus_dockets, k, gmm_dockets.predict(lsa_dockets))\n",
    "#plot_clustering(k, groupings_dockets, titles_dockets, 'Docket Entries 2015-18')\n",
    "\n",
    "# Cluster via cosine/hiearchical\n",
    "# Best silhouette score is found with k=3, with 5 also being a local maximum\n",
    "# Plot leading to this conclusion commented out below\n",
    "k = 4\n",
    "linkage_dockets = hierarchy.linkage(lsa_dockets, \"average\", metric=\"cosine\")\n",
    "hier_dockets = hierarchy.fcluster(linkage_dockets, k, criterion='maxclust') - 1\n",
    "groupings_dockets, titles_dockets = extract_groupings(lsa_dockets, corpus_dockets, k, hier_dockets)\n",
    "plot_clustering(k, groupings_dockets, titles_dockets, 'Cosine Similarity of Docket Entries')\n",
    "\n",
    "# Without labeling, we may graph this way\n",
    "#_ = plt.scatter(lsa_dockets[:,0], lsa_dockets[:,1], c=gmm_dockets.predict(lsa_dockets))\n",
    "\n",
    "# This is the code we would use to display silhouette scores per k for GMM\n",
    "#x = list(range(2,15))\n",
    "#y = []\n",
    "#for i in x:\n",
    "#    gmm = mixture.GaussianMixture(n_components=i).fit(lsa_dockets)\n",
    "#    y.append(metrics.silhouette_score(lsa_dockets, gmm.predict(lsa_dockets)))\n",
    "#_ = plt.plot(x, y)\n",
    "\n",
    "# This is the code we would use to display silhouette scores per k for cosine\n",
    "#x = list(range(2,15))\n",
    "#y = []\n",
    "#linkage_dockets = hierarchy.linkage(lsa_dockets, \"average\", metric=\"cosine\")\n",
    "#for i in x:\n",
    "#    hier = hierarchy.fcluster(linkage_dockets, i, criterion='maxclust') - 1\n",
    "#    y.append(metrics.silhouette_score(lsa_dockets, hier))\n",
    "#_ = plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([re.sub(r'\\d', '', case['Docket Entries']).split('%%%')[-2] for case in cases if 'Docket Entries' in case and len(case['Docket Entries']) > 0 and 'Case Type' in case and case['Case Type'] == 'Criminal' and (case['Case Status'].find('Rescript') >= 0) and case['Docket Entries'].lower().find('verdict') >= 0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be where we put code to analyze/cluster our Lexis opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_lexis_page(filename):\n",
    "    \"\"\"\n",
    "    Open the Lexis html file and Soup it as beautifully as possible\n",
    "    \n",
    "    Input:\n",
    "        filename: The filename to parse\n",
    "    Output:\n",
    "        A dictionary of the items found in the case page\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(filename, 'rb'), 'html.parser')\n",
    "    info = {}\n",
    "    \n",
    "    # Get document text\n",
    "    doctext = soup.find(\"div\", {\"class\": \"document-text\"})\n",
    "    # TODO: Figure out why some documents return None from the previous step\n",
    "    if not doctext:\n",
    "        return {}\n",
    "    \n",
    "    # Parse metadata\n",
    "    title = doctext.find(\"h1\", {\"id\": \"SS_DocumentTitle\"}).text.strip()\n",
    "    docinfo = doctext.find_all(\"p\", {\"class\": \"SS_DocumentInfo\"})\n",
    "    court = docinfo[0].text.strip()\n",
    "    dates = docinfo[1].text.strip().split(';')\n",
    "    case = docinfo[2].text.strip()\n",
    "    info['Case Title'] = title\n",
    "    info['Court'] = court\n",
    "    # TODO: Fix date parsing\n",
    "    #info['Date Argued'] = dates[0]\n",
    "    #info['Date Decided'] = dates[1]\n",
    "    info['Case Number'] = case\n",
    "    reporter = []\n",
    "    for sp in doctext.find_all(\"span\", {\"class\": \"SS_NonPaginatedRptr\"}):\n",
    "        reporter.append(sp.text.strip())\n",
    "    # TODO: Get more from this section\n",
    "    info['Reporter'] = ' | '.join(reporter)\n",
    "    # TODO: Find out how to parse Prior History and similar (e.g. subsequent history)\n",
    "    #prior = doctext.find_all(\"p\", {\"class\": \"SS_InlineText\"})[-1].text\n",
    "    #prior = re.sub(r\"\\s+\", \" \", prior, flags=re.UNICODE)\n",
    "    #info['Prior History'] = prior\n",
    "    \n",
    "    \n",
    "    #start = doctext.find(\"span\", id=\"JUMPTO_Counsel\")\n",
    "    #for i in range(25):\n",
    "    #    print(str(start.next_sibling).strip())\n",
    "    #    start = start.next_sibling\n",
    "    #here get stuff until br (end of category) and span (end of section)\n",
    "    \n",
    "    # TODO: Parse headnotes\n",
    "    \n",
    "    # Parse opinions\n",
    "    info['Opinion Author'] = get_text_after_span(doctext, \"JUMPTO_Opinionby\")\n",
    "    info['Opinion'] = get_text_after_id(doctext, \"JUMPTO_Opinion\")\n",
    "    info['Concuring Opinion Author'] = get_text_after_span(doctext, \"JUMPTO_Concurby\")\n",
    "    info['Concurring Opinion'] = get_text_after_id(doctext, \"JUMPTO_Concur\")\n",
    "    info['Dissenting Opinion Author'] = get_text_after_span(doctext, \"JUMPTO_Dissentby\")\n",
    "    info['Dissenting Opinion'] = get_text_after_id(doctext, \"JUMPTO_Dissent\")\n",
    "    \n",
    "    return info\n",
    "\n",
    "def get_text_after_span(document, s_id):\n",
    "    \"\"\"\n",
    "    Get the text immediately following some span with given id\n",
    "    \n",
    "    Input:\n",
    "        document: The section of text\n",
    "        s_id: The id of the span\n",
    "    Output:\n",
    "        The text immediately following said element (or the empty string if the id does not exist)\n",
    "    \"\"\"\n",
    "    start = document.find(\"span\", id=s_id)\n",
    "    if not start:\n",
    "        return \"\"\n",
    "    return str(start.next_sibling).strip()\n",
    "\n",
    "def get_text_after_id(document, e_id):\n",
    "    \"\"\"\n",
    "    Get the text in the paragraphs immediately following some element with given id\n",
    "    \n",
    "    Input:\n",
    "        document: The section of text\n",
    "        e_id: The id of the span\n",
    "    Output:\n",
    "        The text immediately following said element (or the empty string if the id does not exist)\n",
    "    \"\"\"\n",
    "    start = document.find(id=e_id)\n",
    "    if not start:\n",
    "        return \"\"\n",
    "    element = start.next_sibling\n",
    "    ps = []\n",
    "    while element and element.name == 'p':\n",
    "        text = element.text.strip()\n",
    "        text = re.sub(r\"\\s+\", \" \", text, flags=re.UNICODE)\n",
    "        ps.append(text)\n",
    "        element = element.next_sibling\n",
    "    return \" %%% \".join(ps)\n",
    "\n",
    "#base = \"http://www.ma-appellatecourts.org/display_docket.php?src=party&dno=\"\n",
    "folder = 'Reversal Opinions HTML'\n",
    "cases = []\n",
    "keys = set([])\n",
    "\n",
    "# Read in all the downloaded pages and print / process them\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".html\"):\n",
    "        fullname = os.path.join(folder, file)\n",
    "        case = scrape_lexis_page(fullname)\n",
    "        keys.update(case.keys())\n",
    "        cases.append(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import mixture\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "corpus_opinions = [case['Opinion'] for case in cases if 'Opinion' in case]\n",
    "\n",
    "# Compute the tf-idf scores of the opinions\n",
    "fe_tfv = TfidfVectorizer(stop_words='english', min_df = 0.01, max_df = 0.5)\n",
    "tfidf_opinions = fe_tfv.fit_transform(corpus_opinions)\n",
    "\n",
    "# Compute the LSA of the scored opinions\n",
    "# After 4 components, we don't get much more ROI (plot leading to this conclusion is commented out below)\n",
    "# min/max document frequency play a huge role here\n",
    "n_c = 4\n",
    "dc_tsvd = TruncatedSVD(n_components=n_c)\n",
    "lsa_opinions = dc_tsvd.fit_transform(tfidf_opinions)\n",
    "\n",
    "# This is the code we would use to graph the singular values\n",
    "#fe_tfv = TfidfVectorizer(stop_words='english', min_df = 0.01, max_df = 0.5)\n",
    "#tfidf_opinions = fe_tfv.fit_transform(corpus_opinions)\n",
    "#dc_tsvd = TruncatedSVD(n_components=50)\n",
    "#lsa_opinions = dc_tsvd.fit_transform(tfidf_opinions)\n",
    "#plt.plot(range(1,51), dc_tsvd.singular_values_)\n",
    "#print(dc_tsvd.singular_values_)\n",
    "\n",
    "# Defining terms for each component\n",
    "#terms = fe_tfv.get_feature_names()\n",
    "#for i in range(n_c):\n",
    "#    top = np.argsort(dc_tsvd.components_[i])\n",
    "#    topterms = [terms[top[f]] for f in range(25)]\n",
    "#    print (i, topterms)\n",
    "\n",
    "# Cluster via GMM\n",
    "# Best silhouette score is found with k=2, and k=5 is the next local optimum\n",
    "# Opting for 5 to obtain more semantic criteria for reversal\n",
    "# Plot leading to this conclusion commented out below\n",
    "gmm_opinions = mixture.GaussianMixture(n_components=5).fit(lsa_opinions)\n",
    "_ = plt.scatter(lsa_opinions[:,0], lsa_opinions[:,1], c=gmm_opinions.predict(lsa_opinions))\n",
    "\n",
    "# This is the code we would use to display silhouette scores per k\n",
    "#x = list(range(2,15))\n",
    "#y = []\n",
    "#for i in x:\n",
    "#    gmm = mixture.GaussianMixture(n_components=i).fit(lsa_opinions)\n",
    "#    y.append(metrics.silhouette_score(lsa_opinions, gmm.predict(lsa_opinions)))\n",
    "#plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodecsv as csv\n",
    "\n",
    "print(keys)\n",
    "\n",
    "# Write out the csv\n",
    "with open('sjc-opinions.csv', 'wb') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
